{"cells":[{"cell_type":"markdown","metadata":{},"source":["## INITIALIZE LOGGER"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:11.915579Z","iopub.status.busy":"2024-05-23T19:54:11.915288Z","iopub.status.idle":"2024-05-23T19:54:14.590962Z","shell.execute_reply":"2024-05-23T19:54:14.590039Z","shell.execute_reply.started":"2024-05-23T19:54:11.915554Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login(key=\"3db31cd19d063689e924d07069de6c7a1670642b\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:14.593070Z","iopub.status.busy":"2024-05-23T19:54:14.592295Z","iopub.status.idle":"2024-05-23T19:54:31.493201Z","shell.execute_reply":"2024-05-23T19:54:31.492273Z","shell.execute_reply.started":"2024-05-23T19:54:14.593037Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manushka16\u001b[0m (\u001b[33msequential_meta_classifier\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240523_195414-luo3kgpn</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/sequential_meta_classifier/sequential_meta_classifier/runs/luo3kgpn' target=\"_blank\">FINALRUNS</a></strong> to <a href='https://wandb.ai/sequential_meta_classifier/sequential_meta_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/sequential_meta_classifier/sequential_meta_classifier' target=\"_blank\">https://wandb.ai/sequential_meta_classifier/sequential_meta_classifier</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/sequential_meta_classifier/sequential_meta_classifier/runs/luo3kgpn' target=\"_blank\">https://wandb.ai/sequential_meta_classifier/sequential_meta_classifier/runs/luo3kgpn</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sequential_meta_classifier/sequential_meta_classifier/runs/luo3kgpn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a5870fbc8b0>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"sequential_meta_classifier\",\n","    name=\"FINALRUNS\",\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## IMPORTS"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T20:18:38.682232Z","iopub.status.busy":"2024-05-23T20:18:38.681810Z","iopub.status.idle":"2024-05-23T20:18:39.142547Z","shell.execute_reply":"2024-05-23T20:18:39.141313Z","shell.execute_reply.started":"2024-05-23T20:18:38.682197Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import os\n","import json\n","import math\n","from collections import OrderedDict\n","import torch\n","from torch import nn, Tensor\n","from typing import Union, Tuple, List, Iterable, Dict\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.utils.data import DataLoader\n","import random\n","import numpy as np\n","import gzip, csv\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import torch.nn.init as init\n","from torch.utils.data import DataLoader, random_split\n","from transformers import BertTokenizer\n","from datasets import load_dataset\n","from torch.utils.data import Dataset\n","from sklearn.metrics import f1_score\n","torch.manual_seed(0)\n","np.random.seed(0)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Activation Function"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:51.490882Z","iopub.status.busy":"2024-05-23T19:54:51.489328Z","iopub.status.idle":"2024-05-23T19:54:51.497521Z","shell.execute_reply":"2024-05-23T19:54:51.496486Z","shell.execute_reply.started":"2024-05-23T19:54:51.490839Z"},"trusted":true},"outputs":[],"source":["def gelu(x):\n","    \"\"\"Implementation of the gelu activation function.\"\"\"\n","    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n"]},{"cell_type":"markdown","metadata":{},"source":["## POSITIONAL ENCODING LAYER"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:51.500086Z","iopub.status.busy":"2024-05-23T19:54:51.499149Z","iopub.status.idle":"2024-05-23T19:54:51.511109Z","shell.execute_reply":"2024-05-23T19:54:51.509572Z","shell.execute_reply.started":"2024-05-23T19:54:51.500049Z"},"trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, embed_dim: int, drop_rate=0.1, max_len=5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=drop_rate)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n","        pe = torch.zeros(1, max_len, embed_dim)\n","        pe[0, :, 0::2] = torch.sin(position * div_term)\n","        pe[0, :, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n","        \n","        Returns:\n","            torch.Tensor: Output tensor after adding positional encodings and applying dropout.\n","                 It has the same shape as the input tensor [batch_size, seq_len, embedding_dim].\n","                 The positional encodings are added to the input tensor along the sequence length dimension,\n","                 and dropout is applied to the combined tensor.\n","        \n","        \"\"\"\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)"]},{"cell_type":"markdown","metadata":{},"source":["## ATTENTION MECHANISM"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:51.512869Z","iopub.status.busy":"2024-05-23T19:54:51.512230Z","iopub.status.idle":"2024-05-23T19:54:51.526069Z","shell.execute_reply":"2024-05-23T19:54:51.525306Z","shell.execute_reply.started":"2024-05-23T19:54:51.512839Z"},"trusted":true},"outputs":[],"source":["def scaled_dot_product(q, k, v, attn_drop_rate=0.1):\n","    \"\"\"\n","    Args:\n","      q: query, shape: (batch, # heads, seq len, head dimension)\n","      k: keys, shape: (batch, # heads, seq len, head dimension)\n","      v: value, shape: (batch, # heads, seq len, head dimension)\n","      attn_drop_rate: probability of an element to be zeroed,\n","      mask: the optional masking of specific entries in the attention matrix.\n","              shape: (batch, seq len)\n","    \n","     Returns:\n","        torch.Tensor: Output tensor after scaled dot product attention computation.\n","           Shape: (batch, # heads, seq len, head dimension).\n","    \n","    \"\"\"\n","\n","    d_k = q.shape[-1]\n","    attn_logits = torch.matmul(q, k.transpose(-1, -2))\n","    attn_logits = attn_logits/math.sqrt(d_k)\n","    attention = F.softmax(attn_logits, dim=-1)\n","    attention = F.dropout(attention, p=attn_drop_rate)\n","    values = torch.matmul(attention,v)\n","    return values"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:51.528191Z","iopub.status.busy":"2024-05-23T19:54:51.527467Z","iopub.status.idle":"2024-05-23T19:54:51.548295Z","shell.execute_reply":"2024-05-23T19:54:51.547642Z","shell.execute_reply.started":"2024-05-23T19:54:51.528160Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, embed_dim, n_heads, attn_drop_rate):\n","        super().__init__()\n","        self.embed_dim = embed_dim\n","        self.n_heads = n_heads\n","        self.head_dim = embed_dim // n_heads\n","        self.attn_drop_rate = attn_drop_rate\n","        self.query = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n","        self.key = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n","        self.value = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n","        self.o_proj = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n","        self._reset_parameters()\n","\n","    def _reset_parameters(self):\n","      nn.init.xavier_uniform_(self.query.weight)\n","      self.query.bias.data.fill_(0)\n","      nn.init.xavier_uniform_(self.key.weight)\n","      self.key.bias.data.fill_(0)\n","      nn.init.xavier_uniform_(self.value.weight)\n","      self.value.bias.data.fill_(0)\n","      nn.init.xavier_uniform_(self.o_proj.weight)\n","      self.o_proj.bias.data.fill_(0)\n","\n","    def split_heads(self, tensor):\n","       new_shape = tensor.size()[:-1] + (self.n_heads, self.head_dim)\n","       tensor = tensor.view(*new_shape)\n","       tensor = tensor.permute(0, 2, 1, 3).contiguous()\n","       return tensor\n","\n","    def merge_heads(self, tensor, batch_size, seq_length):\n","       tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n","       return tensor\n","\n","    def forward(self, embedding):\n","      \"\"\"\n","       Args:\n","        embedding (torch.Tensor): \n","            A tensor of shape (batch_size, seq_length, embed_dim) representing the input embeddings.\n","            - `batch_size`: The number of samples in the batch.\n","            - `seq_length`: The number of tokens (or time steps) in each sequence.\n","            - `embed_dim`: The dimension of the embedding for each token.\n","       \n","       Returns:\n","        torch.Tensor: \n","            A tensor of shape (batch_size, seq_length, embed_dim) representing the attended embeddings.\n","            - `batch_size`: The number of samples in the batch.\n","            - `seq_length`: The number of tokens (or time steps) in each sequence.\n","            - `embed_dim`: The dimension of the embedding for each token.\n","      \"\"\"\n","      batch_size, seq_length, embed_dim = embedding.size()\n","      q, k, v = self.query(embedding), self.key(embedding), self.value(embedding)\n","      q = self.split_heads(q)\n","      k = self.split_heads(k)\n","      v = self.split_heads(v)\n","      values = scaled_dot_product(q, k, v, self.attn_drop_rate)\n","      values = self.merge_heads(values, batch_size, seq_length)\n","      attended_embeds = self.o_proj(values)\n","      return attended_embeds"]},{"cell_type":"markdown","metadata":{},"source":["## LAYER NORMALIZATION LAYER"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:51.554250Z","iopub.status.busy":"2024-05-23T19:54:51.553657Z","iopub.status.idle":"2024-05-23T19:54:51.573361Z","shell.execute_reply":"2024-05-23T19:54:51.572385Z","shell.execute_reply.started":"2024-05-23T19:54:51.554216Z"},"trusted":true},"outputs":[],"source":["class LayerNormalization(nn.Module):\n","    def __init__(self, parameters_shape, eps=1e-5):\n","        super().__init__()\n","        self.parameters_shape=parameters_shape\n","        self.eps=eps\n","        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n","        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","         Args:\n","         inputs (Tensor): Input tensor to normalize.\n","         \n","         Returns:\n","                torch.Tensor: Normalized tensor after applying layer normalization.\n","                 It has the same shape as the input tensor `(batch_size, *parameters_shape)`.\n","\n","        \"\"\"\n","        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n","        mean = inputs.mean(dim=dims, keepdim=True)\n","        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n","        std = (var + self.eps).sqrt()\n","        y = (inputs - mean) / std\n","        out = self.gamma * y  + self.beta\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## FEEDFORWARD LAYER"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:51.577821Z","iopub.status.busy":"2024-05-23T19:54:51.577462Z","iopub.status.idle":"2024-05-23T19:54:51.590685Z","shell.execute_reply":"2024-05-23T19:54:51.589314Z","shell.execute_reply.started":"2024-05-23T19:54:51.577790Z"},"trusted":true},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module): \n","\n","    def __init__(self, embed_dim, drop_prob=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.linear1 = nn.Linear(embed_dim, 4*embed_dim)\n","        self.linear2 = nn.Linear(4*embed_dim, embed_dim)\n","        self.gelu = nn.GELU()\n","        self.dropout = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x):\n","        \"\"\"\n","         Args:\n","             x (torch.Tensor): Input tensor to the feedforward network.\n","                 Its shape should be `(batch_size, sequence_length, embed_dim)`.\n","                 `batch_size` is the number of sequences in a batch,\n","                 `sequence_length` is the length of each sequence,\n","                 and `embed_dim` is the dimensionality of the input and output embeddings.\n","     \n","         Returns:\n","             torch.Tensor: Output tensor of the feedforward network.\n","                 It has the same shape as the input tensor `(batch_size, sequence_length, embed_dim)`.\n","        \n","        \"\"\"\n","        x = self.linear1(x)\n","        x = self.gelu(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## CLASSIFIER LAYER"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:51.593047Z","iopub.status.busy":"2024-05-23T19:54:51.592204Z","iopub.status.idle":"2024-05-23T19:54:51.607293Z","shell.execute_reply":"2024-05-23T19:54:51.606069Z","shell.execute_reply.started":"2024-05-23T19:54:51.592963Z"},"trusted":true},"outputs":[],"source":["class Classifier(nn.Module):\n","    def __init__(self, input_dim, numclasses, dropout_rate=0.1):\n","        super(Classifier, self).__init__()\n","        self.linear = nn.Linear(input_dim, numclasses) \n","\n","    def forward(self, x):\n","     \"\"\"\n","        Args:\n","            x (torch.Tensor): Input tensor to the classifier.\n","                Its shape should be `()`.\n","                `batch_size` is the number of samples in the batch,\n","                and `input_dim` is the dimensionality of the input features.\n","\n","        Returns:\n","            torch.Tensor: Output tensor representing the logits for each class.\n","                It has the shape `()`.\n","                `batch_size` is the number of samples in the batch,\n","                and `num_classes` is the number of classes in the classification task.\n","     \"\"\"\n","     x = self.linear(x)\n","     return x"]},{"cell_type":"markdown","metadata":{},"source":["## ENCODER LAYER"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:51.609019Z","iopub.status.busy":"2024-05-23T19:54:51.608666Z","iopub.status.idle":"2024-05-23T19:54:51.622513Z","shell.execute_reply":"2024-05-23T19:54:51.621459Z","shell.execute_reply.started":"2024-05-23T19:54:51.608991Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","\n","    def __init__(self, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n","        super(EncoderLayer, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.n_heads = n_heads\n","        self.attention = MultiHeadAttention(self.embed_dim, self.n_heads, attn_drop_rate)\n","        self.norm1 = LayerNormalization(parameters_shape=[self.embed_dim])\n","        self.dropout1 = nn.Dropout(p=layer_drop_rate)\n","        self.ffn = PositionwiseFeedForward(self.embed_dim,layer_drop_rate)\n","        self.norm2 = LayerNormalization(parameters_shape=[self.embed_dim])\n","        self.dropout2 = nn.Dropout(p=layer_drop_rate)\n","\n","    def forward(self, x):\n","     \"\"\"\n","        Args:\n","        x (torch.Tensor): Input tensor to the encoder layer.\n","            Its shape should be `(batch_size, seq_length, embed_dim)`.\n","            - `batch_size`: The number of samples in the batch.\n","            - `seq_length`: The number of tokens (or time steps) in each sequence.\n","            - `embed_dim`: The dimension of the embedding for each token.\n","            \n","        Returns:\n","            torch.Tensor: Output tensor representing the encoded representations.\n","                It has the same shape as the input tensor `(batch_size, seq_length, embed_dim)`.\n","                - `batch_size`: The number of samples in the batch.\n","                - `seq_length`: The number of tokens (or time steps) in each sequence.\n","                - `embed_dim`: The dimension of the embedding for each token.\n","            \n","     \"\"\"\n","     residual_x = x\n","     x = self.attention(x)\n","     x = self.dropout1(x)\n","     x = x + residual_x\n","     x = self.norm1(x)\n","     residual_x = x\n","     x = self.ffn(x)\n","     x = self.dropout2(x)\n","     x = x + residual_x\n","     x = self.norm2(x)\n","     return x"]},{"cell_type":"markdown","metadata":{},"source":["## TRANSFORMER LAYER"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T19:54:51.624767Z","iopub.status.busy":"2024-05-23T19:54:51.623819Z","iopub.status.idle":"2024-05-23T19:54:51.638455Z","shell.execute_reply":"2024-05-23T19:54:51.636566Z","shell.execute_reply.started":"2024-05-23T19:54:51.624736Z"},"trusted":true},"outputs":[],"source":["class ENCTransformer(nn.Module): \n","    def __init__(self, n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate, seq_len):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size+1, embed_dim)\n","        self.position = PositionalEncoding(embed_dim, layer_drop_rate)\n","        self.net = nn.Sequential(*[\n","        EncoderLayer(embed_dim, n_heads, attn_drop_rate, layer_drop_rate) for _ in range(n_layers)\n","        ])\n","        self.pooler = nn.Sequential(OrderedDict([\n","            ('dense', nn.Linear(embed_dim, embed_dim)),\n","            ('activation', nn.Tanh()),\n","        ]))\n","        self.classifier = Classifier(embed_dim, num_classes)\n","        self.saved_sample = None\n","        self.seq_len = seq_len\n","\n","    def forward(self, batch_text):\n","     \"\"\"\n","        Args:\n","            batch_text (torch.Tensor): Batch of input texts represented as token indices.\n","                Its shape should be `(batch_size, seq_length)`.\n","        Returns:\n","            torch.Tensor: Predicted logits for each class.\n","                It has the shape `(batch_size, num_classes)`.\n","                - `batch_size`: The number of samples in the batch.\n","                - `num_classes`: The number of classes in the classification task.\n","     \"\"\"\n","     batch_text = batch_text.squeeze(1)\n","     embedding = self.position(self.embed(batch_text)) \n","     new_embedding = self.net((embedding))\n","     o = self.pooler(new_embedding[:, 0])\n","     preds = self.classifier(o)\n","     return preds\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## DATALOADER"]},{"cell_type":"markdown","metadata":{},"source":["### CHOOSE DATASET & DEFINE SEQUENCE LENGTH"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = \"asc\"\n","seq_len = 256"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T20:49:07.019373Z","iopub.status.busy":"2024-05-23T20:49:07.018691Z","iopub.status.idle":"2024-05-23T20:49:07.302803Z","shell.execute_reply":"2024-05-23T20:49:07.301573Z","shell.execute_reply.started":"2024-05-23T20:49:07.019343Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n","class MyDataset(Dataset):\n","    def __init__(self, dataset, tokenizer):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","    @staticmethod\n","    def preprocess_text(text):\n","        # Lowercase the text\n","        text = text.lower()\n","        # Remove URLs\n","        text = re.sub(r'http\\S+|www.\\S+', ' ', text)\n","        # Remove emails\n","        text = re.sub(r'\\S*@\\S*\\s?', ' ', text)\n","        # Remove special characters (keeping letters, numbers, and basic punctuation)\n","        text = re.sub(r'[^a-z0-9,.!? ]', ' ', text)\n","        return text\n","\n","    def __getitem__(self, idx):\n","        item = self.dataset[idx]\n","        text = MyDataset.preprocess_text(item['text'])\n","        encoding = self.tokenizer(text, truncation=True, padding='max_length', return_tensors='pt', max_length=seq_len)\n","        encoding['label'] = torch.tensor(item['label'])\n","        return encoding\n","\n","\n","if df == \"news\":\n","    dataset = load_dataset(\"setfit/20_newsgroups\")\n","elif df == \"asc\":\n","    path=\"/kaggle/input/dsc-dataset/dat/absa/\"\n","    dataset={'train':[],'test':[],'val':[]}\n","    idx=0\n","    for subdir in os.listdir(path):\n","        if subdir!='XuSemEval':\n","            subdir_path = os.path.join(path, subdir+'/asc')\n","            for subsubdir in os.listdir(subdir_path):\n","                subsubdir_path=os.path.join(subdir_path, subsubdir)\n","                train_path = os.path.join(subsubdir_path, 'train.json')\n","                test_path = os.path.join(subsubdir_path, 'test.json')\n","                val_path = os.path.join(subsubdir_path, 'dev.json')\n","                paths=[train_path,test_path,val_path]\n","                for i in range(len(paths)):\n","                    with open(paths[i], 'r') as f:\n","                        l=dataset[list(dataset.keys())[i]]\n","                        data = json.load(f)\n","                        for entry in data.values():\n","                            if \"sentence\" in entry:\n","                                l.append({'text':entry[\"sentence\"],'label':idx})\n","                idx+=1\n","        else:\n","            subdir_path = os.path.join(path, subdir+'/asc')\n","            flag=False\n","            for subsubdir in os.listdir(subdir_path):\n","                subsubdir_path=os.path.join(subdir_path, subsubdir)\n","                if subsubdir=='14':\n","                    flag=True\n","                for subsubsubdir in os.listdir(subsubdir_path):\n","                    subsubsubdir_path=os.path.join(subsubdir_path, subsubsubdir)\n","                    if subsubsubdir=='rest':\n","                        continue\n","                    train_path = os.path.join(subsubsubdir_path, 'train.json')\n","                    test_path = os.path.join(subsubsubdir_path, 'test.json')\n","                    val_path = os.path.join(subsubsubdir_path, 'dev.json')\n","                    paths=[train_path,test_path,val_path]\n","                    for i in range(len(paths)):\n","                        with open(paths[i], 'r') as f:\n","                            l=dataset[list(dataset.keys())[i]]\n","                            data = json.load(f)\n","                            for entry in data.values():\n","                                if flag:\n","                                  if \"sentence\" in entry:\n","                                            l.append({'text':entry[\"sentence\"],'label':idx}) \n","                                else:\n","                                    if entry is not None:\n","                                        for subentry in entry.values():\n","                                            if \"sentence\" in subentry:\n","                                                l.append({'text':subentry[\"sentence\"],'label':idx})                \n","                    idx+=1\n","                flag=False\n","elif df == \"dsc\":\n","    path = \"/kaggle/input/dsc-dataset/dat/dsc/\"\n","    dataset={'train':[],'test':[],'val':[]}\n","    idx=0\n","    for subdir in os.listdir(path):\n","        subdir_path = os.path.join(path, subdir)\n","        if os.path.isdir(subdir_path):\n","            file_count = sum(1 for f in os.listdir(subdir_path) if f.endswith('.json'))\n","            if file_count > 3:\n","                train_path = os.path.join(subdir_path, 'train.json')\n","                test_path = os.path.join(subdir_path, 'test.json')\n","                val_path = os.path.join(subdir_path, 'dev.json')\n","                paths=[train_path,test_path,val_path]\n","                for i in range(len(paths)):\n","                    with open(paths[i], 'r') as f:\n","                        l=dataset[list(dataset.keys())[i]]\n","                        data = json.load(f)\n","                        for entry in data.values():\n","                            if \"sentence\" in entry:\n","                                l.append({'text':entry[\"sentence\"],'label':idx})\n","                idx+=1\n","\n","\n","    \n","    \n","traindata = MyDataset(dataset=dataset['train'], tokenizer=tokenizer)\n","val_size = int(len(traindata) * 0.2)  \n","train_size = len(traindata) - val_size  \n","traindata, valdata = random_split(traindata, [train_size, val_size])\n","testdata = MyDataset(dataset=dataset['test'], tokenizer=tokenizer)\n","train_dataloader = DataLoader(traindata, batch_size=32, shuffle=True)\n","val_dataloader = DataLoader(valdata, batch_size=32)\n","test_dataloader = DataLoader(testdata, batch_size=32)"]},{"cell_type":"markdown","metadata":{},"source":["## CONFIG"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T20:47:13.868139Z","iopub.status.busy":"2024-05-23T20:47:13.867776Z","iopub.status.idle":"2024-05-23T20:47:14.237353Z","shell.execute_reply":"2024-05-23T20:47:14.236045Z","shell.execute_reply.started":"2024-05-23T20:47:13.868112Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<torch.optim.lr_scheduler.CosineAnnealingLR at 0x7a55718030a0>"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["embed_dim = 512\n","n_heads = 8\n","n_layers = 4\n","vocab_size = 30522\n","#vocab_size = 91015\n","attn_drop_rate = 0.5\n","layer_drop_rate = 0.2\n","if df == \"asc\":\n","    num_classes = 18\n","elif df == \"dsc\":\n","    num_classes = 10   \n","elif df == \"news\":\n","    num_classes = 20\n","else:\n","    raise Exception\n","num_epochs = 40\n","model = ENCTransformer(n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate, seq_len)\n","model = model.to(device)\n","\n","def calculate_accuracy(outputs, labels):\n","    _, predicted = torch.max(outputs, dim=1)\n","    correct = (predicted == labels).sum().item()  \n","    total = labels.size(0)\n","    accuracy = correct / total\n","    predicted = predicted.detach().cpu().numpy()\n","    labels = labels.detach().cpu().numpy()\n","    f1macro = f1_score(labels, predicted, average='macro')\n","    return accuracy, f1macro, predicted, labels\n","\n","\n","optimizer = AdamW(model.parameters(), lr=0.0001)\n","criterion = nn.CrossEntropyLoss()\n","torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)"]},{"cell_type":"markdown","metadata":{},"source":["## TRAINING - VALIDATION LOOP"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T20:47:27.718832Z","iopub.status.busy":"2024-05-23T20:47:27.718093Z","iopub.status.idle":"2024-05-23T20:47:40.378603Z","shell.execute_reply":"2024-05-23T20:47:40.377135Z","shell.execute_reply.started":"2024-05-23T20:47:27.718800Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["######## Training Epoch 1/40 #########\n","Batch 10/934 - Avg Loss: 2.5628\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[61], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m calculate_accuracy(output, labels)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#         output = output.detach().cpu().numpy()\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#         output = np.array(output.cpu().detach().numpy())\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#         lables = np.array(lables.cpu().detach().numpy())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#         output = torch.from_numpy(output).to('cuda')\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#         lables = torch.from_numpy(lables).to('cuda')\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for epoch in range(num_epochs):\n","    wandb.log({\"epoch\": epoch+1})\n","    print(f\"######## Training Epoch {epoch + 1}/{num_epochs} #########\")\n","    model.train()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    epoch_f1macro_pred = []\n","    valid_epoch_f1macro_pred = []\n","    all_labels = []\n","    valid_labels = []\n","\n","    for batch_idx,batch in enumerate(train_dataloader,1):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].squeeze(1)\n","        labels = batch['label']\n","        input_ids = input_ids.to(device)\n","        labels = labels.to(device)\n","        output = model(input_ids)\n","        loss = criterion(output, labels)\n","        wandb.log({\"batch_loss\": loss})\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        accuracy, f1macro, predicted, macro_labels = calculate_accuracy(output, labels)\n","        epoch_f1macro_pred.extend(predicted)\n","        all_labels.extend(macro_labels)\n","        total_correct += (accuracy * labels.size(0))\n","        total_samples += labels.size(0)\n","        wandb.log({\"batch_accuracy\": accuracy})\n","        wandb.log({\"batch_f1macro\": f1macro})\n","        if batch_idx % 10 == 0:\n","            avg_loss = total_loss / batch_idx\n","            print(f\"Batch {batch_idx}/{len(train_dataloader)} - Avg Loss: {avg_loss:.4f}\")\n","            wandb.log({\"avg_batch_loss\": avg_loss})\n","    \n","    epoch_f1macro = f1_score(all_labels, epoch_f1macro_pred, average='macro')\n","    print(f\"epoch_f1macro = {epoch_f1macro}\")\n","    avg_epoch_loss = total_loss / len(train_dataloader)\n","    training_accuracy = total_correct / total_samples\n","    print(f\"Epoch {epoch + 1}/{num_epochs} accuracy = {training_accuracy}\")\n","    print(f\"Epoch {epoch + 1} - Avg Loss: {avg_epoch_loss:.4f}\")\n","    wandb.log({\"avg_epoch_loss\": avg_epoch_loss})\n","    wandb.log({\"epoch_training_accuracy\": training_accuracy})\n","    wandb.log({\"epoch_training_f1macro\": epoch_f1macro})\n","    print(\"###### Validating ######\")\n","    model.eval()  \n","    total_correct = 0\n","    total_samples = 0\n","    with torch.no_grad():\n","      for batch in val_dataloader:\n","          input_ids = batch['input_ids'].squeeze(1)\n","          labels = batch['label']\n","          input_ids = input_ids.to(device)\n","          labels = labels.to(device)\n","          outputs = model(input_ids)\n","          accuracy, f1macro, predicted, macro_labels = calculate_accuracy(outputs, labels)\n","          valid_epoch_f1macro_pred.extend(predicted)\n","          valid_labels.extend(macro_labels)\n","          total_correct += (accuracy * labels.size(0))\n","          total_samples += labels.size(0)\n","    valid_f1macro = f1_score(valid_labels, valid_epoch_f1macro_pred, average='macro')\n","    testing_accuracy = total_correct / total_samples\n","    print(f\"Epoch {epoch + 1} - Validation Accuracy: {testing_accuracy:.4f}\")\n","    print(f\"Epoch {epoch + 1} - Validation F1 Macro: {valid_f1macro}\")\n","    wandb.log({\"validation_accuracy\": testing_accuracy})\n","    wandb.log({\"validation_f1macro\": valid_f1macro})\n","\n","    # SAVE MODEL FOE EVERY EPOCH\n","    # curent_state = {\n","    #         'epoch': epoch + 1,\n","    #         'model_state': model.state_dict(),\n","    #         'optimizer_state': optimizer.state_dict(),\n","    #     }\n","    # save_path = f\"models/cross_attention_endoffset_best_model_v14.pth\"\n","    # torch.save(curent_state, save_path)\n","    # print(f\"Saved model state to'{save_path}'\")"]},{"cell_type":"markdown","metadata":{},"source":["## TESTING LOOP"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T20:00:17.126118Z","iopub.status.busy":"2024-05-23T20:00:17.125748Z","iopub.status.idle":"2024-05-23T20:01:57.384375Z","shell.execute_reply":"2024-05-23T20:01:57.383500Z","shell.execute_reply.started":"2024-05-23T20:00:17.126091Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###### Final Testing on Test data ######\n","Final Testing Accuracy: 0.0424\n"]}],"source":["print(\"###### Final Testing on Test data ######\")\n","model.eval()\n","total_correct = 0\n","total_samples = 0\n","test_epoch_f1macro_pred = []\n","test_labels = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        input_ids = batch['input_ids'].squeeze(1)\n","        labels = batch['label']\n","        input_ids = input_ids.to(device)\n","        labels = labels.to(device)\n","        outputs = model(input_ids)\n","\n","        accuracy, f1macro, predicted, macro_labels = calculate_accuracy(outputs, labels)\n","        test_epoch_f1macro_pred.extend(predicted)\n","        test_labels.extend(macro_labels)\n","        total_correct += (accuracy * labels.size(0))\n","        total_samples += labels.size(0)\n","test_f1macro = f1_score(test_labels, test_epoch_f1macro_pred, average='macro')\n","testing_accuracy = total_correct / total_samples\n","print(f\"Final Testing Accuracy: {testing_accuracy:.4f}\")\n","print(f\"Test F1 Macro: {test_f1macro}\")\n","wandb.log({\"testing_f1macro\": test_f1macro})\n","wandb.log({\"testing accuracy\": testing_accuracy})"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5059825,"sourceId":8482928,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
