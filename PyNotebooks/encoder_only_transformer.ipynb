{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"8dd8cac018b63a0686dd945ef666c79145af226b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"sequential_meta_classifier\",\n",
    "    name=\"FINALRUNS\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Union, Tuple, List, Iterable, Dict\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import gzip, csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POSITIONAL ENCODING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim: int, drop_rate=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(1, max_len, embed_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after adding positional encodings and applying dropout.\n",
    "                 It has the same shape as the input tensor [batch_size, seq_len, embedding_dim].\n",
    "                 The positional encodings are added to the input tensor along the sequence length dimension,\n",
    "                 and dropout is applied to the combined tensor.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, attn_drop_rate=0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      q: query, shape: (batch, # heads, seq len, head dimension)\n",
    "      k: keys, shape: (batch, # heads, seq len, head dimension)\n",
    "      v: value, shape: (batch, # heads, seq len, head dimension)\n",
    "      attn_drop_rate: probability of an element to be zeroed,\n",
    "      mask: the optional masking of specific entries in the attention matrix.\n",
    "              shape: (batch, seq len)\n",
    "    \n",
    "     Returns:\n",
    "        torch.Tensor: Output tensor after scaled dot product attention computation.\n",
    "           Shape: (batch, # heads, seq len, head dimension).\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-1, -2))\n",
    "    attn_logits = attn_logits/math.sqrt(d_k)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    attention = F.dropout(attention, p=attn_drop_rate)\n",
    "    values = torch.matmul(attention,v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.query = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.key = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "      nn.init.xavier_uniform_(self.query.weight)\n",
    "      self.query.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.key.weight)\n",
    "      self.key.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.value.weight)\n",
    "      self.value.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "      self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def split_heads(self, tensor):\n",
    "       new_shape = tensor.size()[:-1] + (self.n_heads, self.head_dim)\n",
    "       tensor = tensor.view(*new_shape)\n",
    "       tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "       return tensor\n",
    "\n",
    "    def merge_heads(self, tensor, batch_size, seq_length):\n",
    "       tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "       return tensor\n",
    "\n",
    "    def forward(self, embedding):\n",
    "      \"\"\"\n",
    "       Args:\n",
    "        embedding (torch.Tensor): \n",
    "            A tensor of shape (batch_size, seq_length, embed_dim) representing the input embeddings.\n",
    "            - `batch_size`: The number of samples in the batch.\n",
    "            - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "            - `embed_dim`: The dimension of the embedding for each token.\n",
    "       \n",
    "       Returns:\n",
    "        torch.Tensor: \n",
    "            A tensor of shape (batch_size, seq_length, embed_dim) representing the attended embeddings.\n",
    "            - `batch_size`: The number of samples in the batch.\n",
    "            - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "            - `embed_dim`: The dimension of the embedding for each token.\n",
    "      \"\"\"\n",
    "      batch_size, seq_length, embed_dim = embedding.size()\n",
    "      q, k, v = self.query(embedding), self.key(embedding), self.value(embedding)\n",
    "      q = self.split_heads(q)\n",
    "      k = self.split_heads(k)\n",
    "      v = self.split_heads(v)\n",
    "      values = scaled_dot_product(q, k, v, self.attn_drop_rate)\n",
    "      values = self.merge_heads(values, batch_size, seq_length)\n",
    "      attended_embeds = self.o_proj(values)\n",
    "      return attended_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAYER NORMALIZATION LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "         Args:\n",
    "         inputs (Tensor): Input tensor to normalize.\n",
    "         \n",
    "         Returns:\n",
    "                torch.Tensor: Normalized tensor after applying layer normalization.\n",
    "                 It has the same shape as the input tensor `(batch_size, *parameters_shape)`.\n",
    "\n",
    "        \"\"\"\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y  + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEEDFORWARD LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module): \n",
    "\n",
    "    def __init__(self, embed_dim, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, 4*embed_dim)\n",
    "        self.linear2 = nn.Linear(4*embed_dim, embed_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "         Args:\n",
    "             x (torch.Tensor): Input tensor to the feedforward network.\n",
    "                 Its shape should be `(batch_size, sequence_length, embed_dim)`.\n",
    "                 `batch_size` is the number of sequences in a batch,\n",
    "                 `sequence_length` is the length of each sequence,\n",
    "                 and `embed_dim` is the dimensionality of the input and output embeddings.\n",
    "     \n",
    "         Returns:\n",
    "             torch.Tensor: Output tensor of the feedforward network.\n",
    "                 It has the same shape as the input tensor `(batch_size, sequence_length, embed_dim)`.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, numclasses, dropout_rate=0.1):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, numclasses) \n",
    "\n",
    "    def forward(self, x):\n",
    "     \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor to the classifier.\n",
    "                Its shape should be `()`.\n",
    "                `batch_size` is the number of samples in the batch,\n",
    "                and `input_dim` is the dimensionality of the input features.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor representing the logits for each class.\n",
    "                It has the shape `()`.\n",
    "                `batch_size` is the number of samples in the batch,\n",
    "                and `num_classes` is the number of classes in the classification task.\n",
    "     \"\"\"\n",
    "     x = self.linear(x)\n",
    "     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.attention = MultiHeadAttention(self.embed_dim, self.n_heads, attn_drop_rate)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout1 = nn.Dropout(p=layer_drop_rate)\n",
    "        self.ffn = PositionwiseFeedForward(self.embed_dim,layer_drop_rate)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout2 = nn.Dropout(p=layer_drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "     \"\"\"\n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor to the encoder layer.\n",
    "            Its shape should be `(batch_size, seq_length, embed_dim)`.\n",
    "            - `batch_size`: The number of samples in the batch.\n",
    "            - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "            - `embed_dim`: The dimension of the embedding for each token.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor representing the encoded representations.\n",
    "                It has the same shape as the input tensor `(batch_size, seq_length, embed_dim)`.\n",
    "                - `batch_size`: The number of samples in the batch.\n",
    "                - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "                - `embed_dim`: The dimension of the embedding for each token.\n",
    "            \n",
    "     \"\"\"\n",
    "     residual_x = x\n",
    "     x = self.attention(x)\n",
    "     x = self.dropout1(x)\n",
    "     x = x + residual_x\n",
    "     x = self.norm1(x)\n",
    "     residual_x = x\n",
    "     x = self.ffn(x)\n",
    "     x = self.dropout2(x)\n",
    "     x = x + residual_x\n",
    "     x = self.norm2(x)\n",
    "     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCTransformer(nn.Module): \n",
    "    def __init__(self, n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size+1, embed_dim)\n",
    "        self.position = PositionalEncoding(embed_dim, layer_drop_rate)\n",
    "        self.net = nn.Sequential(*[\n",
    "        EncoderLayer(embed_dim, n_heads, attn_drop_rate, layer_drop_rate) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.pooler = nn.Sequential(OrderedDict([\n",
    "            ('dense', nn.Linear(embed_dim, embed_dim)),\n",
    "            ('activation', nn.Tanh()),\n",
    "        ]))\n",
    "        self.classifier = Classifier(embed_dim, num_classes)\n",
    "        self.saved_sample = None\n",
    "\n",
    "    def forward(self, batch_text):\n",
    "     \"\"\"\n",
    "        Args:\n",
    "            batch_text (torch.Tensor): Batch of input texts represented as token indices.\n",
    "                Its shape should be `(batch_size, seq_length)`.\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted logits for each class.\n",
    "                It has the shape `(batch_size, num_classes)`.\n",
    "                - `batch_size`: The number of samples in the batch.\n",
    "                - `num_classes`: The number of classes in the classification task.\n",
    "     \"\"\"\n",
    "     batch_text = batch_text.squeeze(1)\n",
    "     embedding = self.position(self.embed(batch_text)) \n",
    "     new_embedding = self.net((embedding))\n",
    "     o = self.pooler(new_embedding[:, 0])\n",
    "     preds = self.classifier(o)\n",
    "     return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "#tokenizer = BertTokenizer(vocab_file='CUSTOM_VOCAB/custom_vocab.txt')\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.tokenizer(item['text'], truncation=True, padding='max_length', return_tensors='pt', max_length=512)\n",
    "        encoding['label'] = torch.tensor(item['label'])\n",
    "        return encoding\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"setfit/20_newsgroups\")\n",
    "\n",
    "\n",
    "traindata = MyDataset(dataset=dataset['train'], tokenizer=tokenizer)\n",
    "val_size = int(len(traindata) * 0.2)  \n",
    "train_size = len(traindata) - val_size  \n",
    "traindata, valdata = random_split(traindata, [train_size, val_size])\n",
    "testdata = MyDataset(dataset=dataset['test'], tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(traindata, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(valdata, batch_size=32)\n",
    "test_dataloader = DataLoader(testdata, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "n_heads = 8\n",
    "n_layers = 4\n",
    "vocab_size = 30522\n",
    "#vocab_size = 91015\n",
    "attn_drop_rate = 0.5\n",
    "layer_drop_rate = 0.2\n",
    "num_classes=20\n",
    "num_epochs = 40\n",
    "model = ENCTransformer(n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate)\n",
    "model = model.to(device)\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    correct = (predicted == labels).sum().item()  \n",
    "    total = labels.size(0) \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING - VALIDATION LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    wandb.log({\"epoch\": epoch+1})\n",
    "    print(f\"######## Training Epoch {epoch + 1}/{num_epochs} #########\")\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx,batch in enumerate(train_dataloader,1):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        labels = batch['label']\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output, labels)\n",
    "        wandb.log({\"batch_loss\": loss})\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        accuracy = calculate_accuracy(output, labels)\n",
    "        total_correct += (accuracy * labels.size(0))\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        wandb.log({\"batch_accuracy\": accuracy})\n",
    "\n",
    "        if batch_idx % 10 == 0:  \n",
    "            avg_loss = total_loss / batch_idx\n",
    "            print(f\"Batch {batch_idx}/{len(train_dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
    "            wandb.log({\"avg_batch_loss\": avg_loss})\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(train_dataloader)\n",
    "    training_accuracy = total_correct / total_samples\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} accuracy = {training_accuracy}\")\n",
    "    print(f\"Epoch {epoch + 1} - Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    wandb.log({\"avg_epoch_loss\": avg_epoch_loss})\n",
    "    wandb.log({\"epoch_training_accuracy\": training_accuracy})\n",
    "\n",
    "    print(\"###### Validating ######\")\n",
    "    model.eval()  \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "      for batch in val_dataloader:\n",
    "          input_ids = batch['input_ids'].squeeze(1)\n",
    "          labels = batch['label']\n",
    "          input_ids = input_ids.to(device)\n",
    "          labels = labels.to(device)\n",
    "          outputs = model(input_ids)\n",
    "          accuracy = calculate_accuracy(outputs, labels)\n",
    "          total_correct += (accuracy * labels.size(0))\n",
    "          total_samples += labels.size(0)\n",
    "    testing_accuracy = total_correct / total_samples\n",
    "    print(f\"Epoch {epoch + 1} - Validation Accuracy: {testing_accuracy:.4f}\")\n",
    "    wandb.log({\"validation_accuracy\": testing_accuracy})\n",
    "\n",
    "    # SAVE MODEL FOE EVERY EPOCH\n",
    "    # curent_state = {\n",
    "    #         'epoch': epoch + 1,\n",
    "    #         'model_state': model.state_dict(),\n",
    "    #         'optimizer_state': optimizer.state_dict(),\n",
    "    #     }\n",
    "    # save_path = f\"models/cross_attention_endoffset_best_model_v14.pth\"\n",
    "    # torch.save(curent_state, save_path)\n",
    "    # print(f\"Saved model state to'{save_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"###### Final Testing on Test data ######\")\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        labels = batch['label']\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        accuracy = calculate_accuracy(outputs, labels)\n",
    "        total_correct += (accuracy * labels.size(0))\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "testing_accuracy = total_correct / total_samples\n",
    "print(f\"Final Testing Accuracy: {testing_accuracy:.4f}\")\n",
    "wandb.log({\"testing accuracy\": testing_accuracy})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
