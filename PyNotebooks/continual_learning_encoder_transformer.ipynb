{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"8dd8cac018b63a0686dd945ef666c79145af226b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Union, Tuple, List, Iterable, Dict\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import gzip, csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.init as init\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.benchmarks.generators import nc_benchmark\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.models import MTSimpleMLP\n",
    "from avalanche.training.supervised import EWC\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin\n",
    "from avalanche.logging import WandBLogger\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTIVATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POSTIONAL ENCODING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim: int, drop_rate=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(1, max_len, embed_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after adding positional encodings and applying dropout.\n",
    "                 It has the same shape as the input tensor [batch_size, seq_len, embedding_dim].\n",
    "                 The positional encodings are added to the input tensor along the sequence length dimension,\n",
    "                 and dropout is applied to the combined tensor.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, attn_drop_rate=0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      q: query, shape: (batch, # heads, seq len, head dimension)\n",
    "      k: keys, shape: (batch, # heads, seq len, head dimension)\n",
    "      v: value, shape: (batch, # heads, seq len, head dimension)\n",
    "      attn_drop_rate: probability of an element to be zeroed,\n",
    "      mask: the optional masking of specific entries in the attention matrix.\n",
    "              shape: (batch, seq len)\n",
    "    \n",
    "     Returns:\n",
    "        torch.Tensor: Output tensor after scaled dot product attention computation.\n",
    "           Shape: (batch, # heads, seq len, head dimension).\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-1, -2))\n",
    "    attn_logits = attn_logits/math.sqrt(d_k)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    attention = F.dropout(attention, p=attn_drop_rate)\n",
    "    values = torch.matmul(attention,v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.query = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.key = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "      nn.init.xavier_uniform_(self.query.weight)\n",
    "      self.query.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.key.weight)\n",
    "      self.key.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.value.weight)\n",
    "      self.value.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "      self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def split_heads(self, tensor):\n",
    "       new_shape = tensor.size()[:-1] + (self.n_heads, self.head_dim)\n",
    "       tensor = tensor.view(*new_shape)\n",
    "       tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "       return tensor\n",
    "\n",
    "    def merge_heads(self, tensor, batch_size, seq_length):\n",
    "       tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "       return tensor\n",
    "\n",
    "    def forward(self, embedding):\n",
    "      \"\"\"\n",
    "       Args:\n",
    "        embedding (torch.Tensor): \n",
    "            A tensor of shape (batch_size, seq_length, embed_dim) representing the input embeddings.\n",
    "            - `batch_size`: The number of samples in the batch.\n",
    "            - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "            - `embed_dim`: The dimension of the embedding for each token.\n",
    "       \n",
    "       Returns:\n",
    "        torch.Tensor: \n",
    "            A tensor of shape (batch_size, seq_length, embed_dim) representing the attended embeddings.\n",
    "            - `batch_size`: The number of samples in the batch.\n",
    "            - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "            - `embed_dim`: The dimension of the embedding for each token.\n",
    "      \"\"\"\n",
    "      batch_size, seq_length, embed_dim = embedding.size()\n",
    "      q, k, v = self.query(embedding), self.key(embedding), self.value(embedding)\n",
    "      q = self.split_heads(q)\n",
    "      k = self.split_heads(k)\n",
    "      v = self.split_heads(v)\n",
    "      values = scaled_dot_product(q, k, v, self.attn_drop_rate)\n",
    "      values = self.merge_heads(values, batch_size, seq_length)\n",
    "      attended_embeds = self.o_proj(values)\n",
    "      return attended_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAYER NORMALIZATION LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "         Args:\n",
    "         inputs (Tensor): Input tensor to normalize.\n",
    "         \n",
    "         Returns:\n",
    "                torch.Tensor: Normalized tensor after applying layer normalization.\n",
    "                 It has the same shape as the input tensor `(batch_size, *parameters_shape)`.\n",
    "\n",
    "        \"\"\"\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y  + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEEDFORWARD LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module): \n",
    "\n",
    "    def __init__(self, embed_dim, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, 4*embed_dim)\n",
    "        self.linear2 = nn.Linear(4*embed_dim, embed_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "         Args:\n",
    "             x (torch.Tensor): Input tensor to the feedforward network.\n",
    "                 Its shape should be `(batch_size, sequence_length, embed_dim)`.\n",
    "                 `batch_size` is the number of sequences in a batch,\n",
    "                 `sequence_length` is the length of each sequence,\n",
    "                 and `embed_dim` is the dimensionality of the input and output embeddings.\n",
    "     \n",
    "         Returns:\n",
    "             torch.Tensor: Output tensor of the feedforward network.\n",
    "                 It has the same shape as the input tensor `(batch_size, sequence_length, embed_dim)`.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, numclasses, dropout_rate=0.1):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, numclasses) \n",
    "\n",
    "    def forward(self, x):\n",
    "     \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor to the classifier.\n",
    "                Its shape should be `()`.\n",
    "                `batch_size` is the number of samples in the batch,\n",
    "                and `input_dim` is the dimensionality of the input features.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor representing the logits for each class.\n",
    "                It has the shape `()`.\n",
    "                `batch_size` is the number of samples in the batch,\n",
    "                and `num_classes` is the number of classes in the classification task.\n",
    "     \"\"\"\n",
    "     x = self.linear(x)\n",
    "     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.attention = MultiHeadAttention(self.embed_dim, self.n_heads, attn_drop_rate)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout1 = nn.Dropout(p=layer_drop_rate)\n",
    "        self.ffn = PositionwiseFeedForward(self.embed_dim,layer_drop_rate)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout2 = nn.Dropout(p=layer_drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "     \"\"\"\n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor to the encoder layer.\n",
    "            Its shape should be `(batch_size, seq_length, embed_dim)`.\n",
    "            - `batch_size`: The number of samples in the batch.\n",
    "            - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "            - `embed_dim`: The dimension of the embedding for each token.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor representing the encoded representations.\n",
    "                It has the same shape as the input tensor `(batch_size, seq_length, embed_dim)`.\n",
    "                - `batch_size`: The number of samples in the batch.\n",
    "                - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "                - `embed_dim`: The dimension of the embedding for each token.\n",
    "            \n",
    "     \"\"\"\n",
    "     residual_x = x\n",
    "     x = self.attention(x)\n",
    "     x = self.dropout1(x)\n",
    "     x = x + residual_x\n",
    "     x = self.norm1(x)\n",
    "     residual_x = x\n",
    "     x = self.ffn(x)\n",
    "     x = self.dropout2(x)\n",
    "     x = x + residual_x\n",
    "     x = self.norm2(x)\n",
    "     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCTransformer(nn.Module): \n",
    "    def __init__(self, n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size+1, embed_dim)\n",
    "        self.position = PositionalEncoding(embed_dim, layer_drop_rate)\n",
    "        self.net = nn.Sequential(*[\n",
    "        EncoderLayer(embed_dim, n_heads, attn_drop_rate, layer_drop_rate) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.pooler = nn.Sequential(OrderedDict([\n",
    "            ('dense', nn.Linear(embed_dim, embed_dim)),\n",
    "            ('activation', nn.Tanh()),\n",
    "        ]))\n",
    "        self.classifier = Classifier(embed_dim, num_classes)\n",
    "        self.saved_sample = None\n",
    "\n",
    "    def forward(self, batch_text):\n",
    "     \"\"\"\n",
    "        Args:\n",
    "            batch_text (torch.Tensor): Batch of input texts represented as token indices.\n",
    "                Its shape should be `(batch_size, seq_length)`.\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted logits for each class.\n",
    "                It has the shape `(batch_size, num_classes)`.\n",
    "                - `batch_size`: The number of samples in the batch.\n",
    "                - `num_classes`: The number of classes in the classification task.\n",
    "     \"\"\"\n",
    "     batch_text = batch_text.squeeze(1)\n",
    "     embedding = self.position(self.embed(batch_text)) \n",
    "     new_embedding = self.net((embedding))\n",
    "     o = self.pooler(new_embedding[:, 0])\n",
    "     preds = self.classifier(o)\n",
    "     return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATALOADER & BENCHMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): List of text samples.\n",
    "            labels (list of int): List of labels corresponding to the text samples.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.tokenizer = BertTokenizer(vocab_file='/kaggle/input/custom-dataset-for-ncoder/custom_vocab.txt')\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www.\\S+', ' ', text)\n",
    "        text = re.sub(r'\\S*@\\S*\\s?', ' ', text)\n",
    "        text = re.sub(r'[^a-z0-9,.!? ]', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        text = TextDataset.preprocess_text(text)\n",
    "        encoded_text = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        encoded_text = encoded_text[\"input_ids\"]\n",
    "\n",
    "        return encoded_text, label\n",
    "\n",
    "dataset = load_dataset(\"setfit/20_newsgroups\")\n",
    "\n",
    "train_text = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "\n",
    "test_text = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "train_data = TextDataset(train_text, train_labels)\n",
    "test_data = TextDataset(test_text, test_labels)\n",
    "\n",
    "avl_train_data = AvalancheDataset(train_data)\n",
    "avl_test_data = AvalancheDataset(test_data)\n",
    "\n",
    "\n",
    "avl_train_data.targets = train_labels\n",
    "avl_test_data.targets = test_labels\n",
    "\n",
    "benchmark = nc_benchmark(\n",
    "    test_dataset=avl_test_data,  \n",
    "    train_dataset=avl_train_data,\n",
    "    n_experiences=5,  \n",
    "    task_labels=False  \n",
    ")\n",
    "\n",
    "train_stream = benchmark.train_stream\n",
    "test_stream = benchmark.test_stream\n",
    "experience = train_stream[0]\n",
    "\n",
    "t_label = experience.task_label\n",
    "dataset = experience.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "n_heads = 8\n",
    "n_layers = 4\n",
    "vocab_size = 30522\n",
    "#vocab_size = 91015\n",
    "attn_drop_rate = 0.5\n",
    "layer_drop_rate = 0.2\n",
    "num_classes=20\n",
    "num_epochs = 40\n",
    "batch_size = 32\n",
    "model = ENCTransformer(n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate)\n",
    "model = model.to(device)\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    correct = (predicted == labels).sum().item()  \n",
    "    total = labels.size(0) \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING TESTING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_logger = InteractiveLogger()\n",
    "loggers=[]\n",
    "loggers.append(InteractiveLogger())\n",
    "loggers.append(WandBLogger(project_name=\"sequential_meta_classifier\", run_name=\"40e_0.0001lr_512dim_4h_3l_0.2adr_0.2ldr_AVL_COS_NOMETA\"))\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(\n",
    "        minibatch=False, epoch=True, experience=True, stream=True\n",
    "    ),\n",
    "    confusion_matrix_metrics(num_classes=benchmark.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    loggers=loggers,\n",
    ")\n",
    "\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "\n",
    "\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    criterion, train_mb_size=batch_size, train_epochs=num_epochs, eval_mb_size=batch_size,\n",
    "    evaluator=eval_plugin,device=device,plugins=[replay] )\n",
    "\n",
    "\n",
    "# strategy = EWC(\n",
    "#     model=model,\n",
    "#     optimizer=optimizer,\n",
    "#     criterion=criterion,\n",
    "#     train_mb_size=batch_size,\n",
    "#     train_epochs=num_epochs,\n",
    "#     eval_mb_size=batch_size,\n",
    "#     device=device,\n",
    "#     evaluator=eval_plugin,\n",
    "#     ewc_lambda=0.4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_task in train_stream:\n",
    "    cl_strategy.train(train_task)\n",
    "    cl_strategy.eval(test_stream)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
