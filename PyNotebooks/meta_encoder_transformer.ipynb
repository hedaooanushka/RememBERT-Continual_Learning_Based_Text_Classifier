{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE LOGGER    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"8dd8cac018b63a0686dd945ef666c79145af226b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"sequential_meta_classifier\",\n",
    "    name=\"FINALRUNS\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Union, Tuple, List, Iterable, Dict\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import gzip, csv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTIVATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POSITIONAL ENCODING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim: int, drop_rate=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(1, max_len, embed_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after adding positional encodings and applying dropout.\n",
    "                 It has the same shape as the input tensor [batch_size, seq_len, embedding_dim].\n",
    "                 The positional encodings are added to the input tensor along the sequence length dimension,\n",
    "                 and dropout is applied to the combined tensor.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, attn_drop_rate=0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      q: query, shape: (batch, # heads, seq len, head dimension)\n",
    "      k: keys, shape: (batch, # heads, seq len, head dimension)\n",
    "      v: value, shape: (batch, # heads, seq len, head dimension)\n",
    "      attn_drop_rate: probability of an element to be zeroed,\n",
    "      mask: the optional masking of specific entries in the attention matrix.\n",
    "              shape: (batch, seq len)\n",
    "    \n",
    "     Returns:\n",
    "        torch.Tensor: Output tensor after scaled dot product attention computation.\n",
    "           Shape: (batch, # heads, seq len, head dimension).\n",
    "    \n",
    "    \"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-1, -2))\n",
    "    attn_logits = attn_logits/math.sqrt(d_k)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    attention = F.dropout(attention, p=attn_drop_rate)\n",
    "    values = torch.matmul(attention,v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI HEAD SELF ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.query = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.key = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "      nn.init.xavier_uniform_(self.query.weight)\n",
    "      self.query.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.key.weight)\n",
    "      self.key.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.value.weight)\n",
    "      self.value.bias.data.fill_(0)\n",
    "      nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "      self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def split_heads(self, tensor):\n",
    "       new_shape = tensor.size()[:-1] + (self.n_heads, self.head_dim)\n",
    "       tensor = tensor.view(*new_shape)\n",
    "       tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "       return tensor\n",
    "\n",
    "    def merge_heads(self, tensor, batch_size, seq_length):\n",
    "       tensor = tensor.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "       return tensor\n",
    "\n",
    "    def forward(self, embedding):\n",
    "      \"\"\"\n",
    "       Args:\n",
    "        embedding (torch.Tensor): \n",
    "            A tensor of shape (batch_size, seq_length, embed_dim) representing the input embeddings.\n",
    "            - `batch_size`: The number of samples in the batch.\n",
    "            - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "            - `embed_dim`: The dimension of the embedding for each token.\n",
    "       \n",
    "       Returns:\n",
    "        torch.Tensor: \n",
    "            A tensor of shape (batch_size, seq_length, embed_dim) representing the attended embeddings.\n",
    "            - `batch_size`: The number of samples in the batch.\n",
    "            - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "            - `embed_dim`: The dimension of the embedding for each token.\n",
    "      \n",
    "      \"\"\"\n",
    "      batch_size, seq_length, embed_dim = embedding.size()\n",
    "      q, k, v = self.query(embedding), self.key(embedding), self.value(embedding)\n",
    "      q = self.split_heads(q)\n",
    "      k = self.split_heads(k)\n",
    "      v = self.split_heads(v)\n",
    "      values = scaled_dot_product(q, k, v, self.attn_drop_rate)\n",
    "      values = self.merge_heads(values, batch_size, seq_length)\n",
    "      attended_embeds = self.o_proj(values)\n",
    "      return attended_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI HEAD CROSS ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):  \n",
    "\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.key = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.n_heads*self.head_dim)\n",
    "        self.query = nn.Linear(self.embed_dim , embed_dim)\n",
    "        self.linear_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "   \n",
    "    def forward(self, x, catx): \n",
    "         \"\"\"\n",
    "            Args:\n",
    "             catx (torch.Tensor): \n",
    "                 A tensor of shape (batch_size, seq_length, embed_dim) representing the input embeddings of orignal sequence with additional context.\n",
    "                 - `batch_size`: The number of samples in the batch.\n",
    "                 - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "                 - `embed_dim`: The dimension of the embedding for each token.\n",
    "            \n",
    "             x (torch.Tensor): \n",
    "                 A tensor of shape (batch_size, seq_length, embed_dim) representing the input embeddings of orignal sequence.\n",
    "                 - `batch_size`: The number of samples in the batch.\n",
    "                 - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "                 - `embed_dim`: The dimension of the embedding for each token.     \n",
    "                 \n",
    "            Returns:\n",
    "             torch.Tensor: \n",
    "                 A tensor of shape (batch_size, seq_length, embed_dim) representing the attended embeddings.\n",
    "                 - `batch_size`: The number of samples in the batch.\n",
    "                 - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "                 - `embed_dim`: The dimension of the embedding for each token.\n",
    "        \n",
    "        \"\"\"\n",
    "         qbatch_size, qsequence_length, qembeddings = x.size()\n",
    "         batch_size, sequence_length, embeddings = catx.size()\n",
    "         q, k, v = self.query(x), self.key(catx), self.value(catx)\n",
    "         k = k.reshape(batch_size, sequence_length, self.n_heads, self.head_dim)  \n",
    "         v = v.reshape(batch_size, sequence_length, self.n_heads, self.head_dim)\n",
    "         q = q.reshape(qbatch_size, qsequence_length, self.n_heads, self.head_dim) \n",
    "         k = k.permute(0, 2, 1, 3) \n",
    "         v = v.permute(0, 2, 1, 3)\n",
    "         q = q.permute(0, 2, 1, 3) \n",
    "         values = scaled_dot_product(q, k, v, self.attn_drop_rate) \n",
    "         values = values.reshape(qbatch_size, qsequence_length, embeddings)\n",
    "         out = self.linear_layer(values)  \n",
    "         return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAYER NORMALIZATION LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "         Args:\n",
    "         inputs (Tensor): Input tensor to normalize.\n",
    "         \n",
    "         Returns:\n",
    "                torch.Tensor: Normalized tensor after applying layer normalization.\n",
    "                 It has the same shape as the input tensor `(batch_size, *parameters_shape)`.\n",
    "\n",
    "        \"\"\"\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y  + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEEDFORWARD LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module): \n",
    "\n",
    "    def __init__(self, embed_dim, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, 4*embed_dim)\n",
    "        self.linear2 = nn.Linear(4*embed_dim, embed_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "         Args:\n",
    "             x (torch.Tensor): Input tensor to the feedforward network.\n",
    "                 Its shape should be `(batch_size, sequence_length, embed_dim)`.\n",
    "                 `batch_size` is the number of sequences in a batch,\n",
    "                 `sequence_length` is the length of each sequence,\n",
    "                 and `embed_dim` is the dimensionality of the input and output embeddings.\n",
    "     \n",
    "         Returns:\n",
    "             torch.Tensor: Output tensor of the feedforward network.\n",
    "                 It has the same shape as the input tensor `(batch_size, sequence_length, embed_dim)`.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, numclasses, dropout_rate=0.1):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, numclasses) \n",
    "\n",
    "    def forward(self, x):\n",
    "     \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor to the classifier.\n",
    "                Its shape should be `()`.\n",
    "                `batch_size` is the number of samples in the batch,\n",
    "                and `input_dim` is the dimensionality of the input features.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor representing the logits for each class.\n",
    "                It has the shape `()`.\n",
    "                `batch_size` is the number of samples in the batch,\n",
    "                and `num_classes` is the number of classes in the classification task.\n",
    "     \"\"\"\n",
    "     x = self.linear(x)\n",
    "     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, n_heads, attn_drop_rate, layer_drop_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.crossattention = MultiHeadCrossAttention(self.embed_dim, self.n_heads, attn_drop_rate)\n",
    "        self.attention = MultiHeadAttention(self.embed_dim, self.n_heads, attn_drop_rate)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout1 = nn.Dropout(p=layer_drop_rate)\n",
    "        self.ffn = PositionwiseFeedForward(self.embed_dim,layer_drop_rate)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[self.embed_dim])\n",
    "        self.dropout2 = nn.Dropout(p=layer_drop_rate)\n",
    "\n",
    "    def forward(self, catx, x=None, is_first=False):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "            catx (torch.Tensor): Input tensor to the encoder layer.\n",
    "                Its shape should be `(batch_size, seq_length, embed_dim)`.\n",
    "                - `batch_size`: The number of samples in the batch.\n",
    "                - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "                - `embed_dim`: The dimension of the embedding for each token.\n",
    "    \n",
    "            x (torch.Tensor): Input tensor to the first encoder layer.\n",
    "                Its shape should be `(batch_size, seq_length, embed_dim)`.\n",
    "                - `batch_size`: The number of samples in the batch.\n",
    "                - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "                - `embed_dim`: The dimension of the embedding for each token.\n",
    "                \n",
    "            \n",
    "            Returns:\n",
    "                torch.Tensor: Output tensor representing the encoded representations.\n",
    "                    It has the same shape as the input tensor `(batch_size, seq_length, embed_dim)`.\n",
    "                    - `batch_size`: The number of samples in the batch.\n",
    "                    - `seq_length`: The number of tokens (or time steps) in each sequence.\n",
    "                    - `embed_dim`: The dimension of the embedding for each token.\n",
    "            \n",
    "        \"\"\"\n",
    "        if is_first:\n",
    "            residual_x = x \n",
    "            cross_x = self.crossattention(x, catx)\n",
    "            cross_x = self.dropout1(cross_x)\n",
    "            cross_x = cross_x + residual_x\n",
    "            cross_x = self.norm1(cross_x)\n",
    "            residual_x = cross_x\n",
    "            cross_x = self.ffn(cross_x)\n",
    "            cross_x = self.dropout2(cross_x)\n",
    "            cross_x = cross_x + residual_x\n",
    "            cross_x = self.norm2(cross_x)\n",
    "            return cross_x\n",
    "        \n",
    "        residual_x = catx\n",
    "        catx = self.attention(catx)\n",
    "        catx = self.dropout1(catx)\n",
    "        catx = catx + residual_x\n",
    "        catx = self.norm1(catx)\n",
    "        residual_x = catx\n",
    "        catx = self.ffn(catx)\n",
    "        catx = self.dropout2(catx)\n",
    "        catx = catx + residual_x\n",
    "        catx = self.norm2(catx)\n",
    "        return catx   \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## META TRANSFORMER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rememBERT(nn.Module):\n",
    "    def __init__(self, n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate, seq_len):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size+1, embed_dim)\n",
    "        self.position = PositionalEncoding(embed_dim, layer_drop_rate)\n",
    "        self.first_layer = EncoderLayer(embed_dim, n_heads, attn_drop_rate, layer_drop_rate)\n",
    "        self.net = nn.Sequential(*[\n",
    "        EncoderLayer(embed_dim, n_heads, attn_drop_rate, layer_drop_rate) for _ in range(n_layers-1)\n",
    "        ])\n",
    "        self.pooler = nn.Sequential(OrderedDict([\n",
    "            ('dense', nn.Linear(embed_dim, embed_dim)),\n",
    "            ('activation', nn.Tanh()),\n",
    "        ]))\n",
    "        self.classifier = Classifier(embed_dim, num_classes)\n",
    "        self.seq_len = seq_len \n",
    "        self.saved_sample = None\n",
    "    def forward(self, batch_text):\n",
    "        \"\"\"\n",
    "             Args:\n",
    "                 batch_text (torch.Tensor): Batch of input texts represented as token indices.\n",
    "                     Its shape should be `(batch_size, seq_length)`.\n",
    "             Returns:\n",
    "                 torch.Tensor: Predicted logits for each class.\n",
    "                     It has the shape `(batch_size, num_classes)`.\n",
    "                     - `batch_size`: The number of samples in the batch.\n",
    "                     - `num_classes`: The number of classes in the classification task.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            if self.saved_sample == None:\n",
    "                    self.saved_sample = batch_text[:1]\n",
    "            newtensor = []\n",
    "            for i in range(1):\n",
    "                offset_x = torch.roll(batch_text, shifts=+i+1, dims=0)\n",
    "            zerotensor = torch.zeros_like(batch_text[0])\n",
    "            for i in range(len(batch_text)-1):\n",
    "                if random.random() < 0.3:\n",
    "                    newtensor.append(torch.cat((zerotensor,offset_x[i+1])))\n",
    "                else:\n",
    "                    newtensor.append(torch.cat((offset_x[i],offset_x[i+1])))\n",
    "            newtensor.append(torch.cat((offset_x[-1],offset_x[0])))\n",
    "            numpy_array = np.array([t.cpu().detach().numpy() for t in newtensor])\n",
    "            newtensor = torch.Tensor(numpy_array)\n",
    "            newtensor = newtensor.to(device).to(torch.int64)\n",
    "            embedding = self.position(self.embed(newtensor)) \n",
    "            extracted_tensor = embedding[:, self.seq_len:, :]\n",
    "            embedding = self.first_layer(embedding, extracted_tensor, True) \n",
    "        else:\n",
    "            newtensor = []\n",
    "            for i in range(1):\n",
    "                offset_xt = torch.roll(batch_text, shifts=+i+1, dims=0)\n",
    "            for i in range(len(batch_text)-1): ## DATA LEAKAGE ?\n",
    "                newtensor.append(torch.cat((offset_xt[i],offset_xt[i+1])))\n",
    "            newtensor.append(torch.cat((offset_xt[-1],offset_xt[0])))\n",
    "            newtensor = np.array([t.cpu().detach().numpy() for t in newtensor])\n",
    "            newtensor = torch.Tensor(newtensor)\n",
    "            newtensor = newtensor.to(device).to(torch.int64)\n",
    "            embedding = self.position(self.embed(newtensor))\n",
    "            textracted_tensor = embedding[:, self.seq_len:, :]\n",
    "            embedding = self.first_layer(embedding, textracted_tensor, True)\n",
    "\n",
    "        new_embedding = self.net((embedding))\n",
    "        o = self.pooler(new_embedding[:, 0])\n",
    "        preds = self.classifier(o)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATALOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHOOSE DATASET & DEFINE SEQUENCE LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = \"asc\"\n",
    "seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "#tokenizer = BertTokenizer(vocab_file='custom_vocab.txt')\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.tokenizer(item['text'], truncation=True, padding='max_length', return_tensors='pt', max_length=seq_len)\n",
    "        encoding['label'] = torch.tensor(item['label'])\n",
    "        return encoding\n",
    "\n",
    "\n",
    "if df == \"news\":\n",
    "    dataset = load_dataset(\"setfit/20_newsgroups\")\n",
    "elif df == \"asc\":\n",
    "    path=\"/kaggle/input/dsc-dataset/dat/absa/\"\n",
    "    dataset={'train':[],'test':[],'val':[]}\n",
    "    idx=0\n",
    "    for subdir in os.listdir(path):\n",
    "        if subdir!='XuSemEval':\n",
    "            subdir_path = os.path.join(path, subdir+'/asc')\n",
    "            for subsubdir in os.listdir(subdir_path):\n",
    "                subsubdir_path=os.path.join(subdir_path, subsubdir)\n",
    "                train_path = os.path.join(subsubdir_path, 'train.json')\n",
    "                test_path = os.path.join(subsubdir_path, 'test.json')\n",
    "                val_path = os.path.join(subsubdir_path, 'dev.json')\n",
    "                paths=[train_path,test_path,val_path]\n",
    "                for i in range(len(paths)):\n",
    "                    with open(paths[i], 'r') as f:\n",
    "                        l=dataset[list(dataset.keys())[i]]\n",
    "                        data = json.load(f)\n",
    "                        for entry in data.values():\n",
    "                            if \"sentence\" in entry:\n",
    "                                l.append({'text':entry[\"sentence\"],'label':idx})\n",
    "                idx+=1\n",
    "        else:\n",
    "            subdir_path = os.path.join(path, subdir+'/asc')\n",
    "            flag=False\n",
    "            for subsubdir in os.listdir(subdir_path):\n",
    "                subsubdir_path=os.path.join(subdir_path, subsubdir)\n",
    "                if subsubdir=='14':\n",
    "                    flag=True\n",
    "                for subsubsubdir in os.listdir(subsubdir_path):\n",
    "                    subsubsubdir_path=os.path.join(subsubdir_path, subsubsubdir)\n",
    "                    if subsubsubdir=='rest':\n",
    "                        idx=14\n",
    "                    train_path = os.path.join(subsubsubdir_path, 'train.json')\n",
    "                    test_path = os.path.join(subsubsubdir_path, 'test.json')\n",
    "                    val_path = os.path.join(subsubsubdir_path, 'dev.json')\n",
    "                    paths=[train_path,test_path,val_path]\n",
    "                    for i in range(len(paths)):\n",
    "                        with open(paths[i], 'r') as f:\n",
    "                            l=dataset[list(dataset.keys())[i]]\n",
    "                            data = json.load(f)\n",
    "                            for entry in data.values():\n",
    "                                if flag:\n",
    "                                  if \"sentence\" in entry:\n",
    "                                            l.append({'text':entry[\"sentence\"],'label':idx}) \n",
    "                                else:\n",
    "                                    if entry is not None:\n",
    "                                        for subentry in entry.values():\n",
    "                                            if \"sentence\" in subentry:\n",
    "                                                l.append({'text':subentry[\"sentence\"],'label':idx})                \n",
    "                    idx+=1\n",
    "                flag=False\n",
    "elif df == \"dsc\":\n",
    "    path = \"/kaggle/input/dsc-dataset/dat/dsc/\"\n",
    "    dataset={'train':[],'test':[],'val':[]}\n",
    "    idx=0\n",
    "    for subdir in os.listdir(path):\n",
    "        subdir_path = os.path.join(path, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            file_count = sum(1 for f in os.listdir(subdir_path) if f.endswith('.json'))\n",
    "            if file_count > 3:\n",
    "                train_path = os.path.join(subdir_path, 'train.json')\n",
    "                test_path = os.path.join(subdir_path, 'test.json')\n",
    "                val_path = os.path.join(subdir_path, 'dev.json')\n",
    "                paths=[train_path,test_path,val_path]\n",
    "                for i in range(len(paths)):\n",
    "                    with open(paths[i], 'r') as f:\n",
    "                        l=dataset[list(dataset.keys())[i]]\n",
    "                        data = json.load(f)\n",
    "                        for entry in data.values():\n",
    "                            if \"sentence\" in entry:\n",
    "                                l.append({'text':entry[\"sentence\"],'label':idx})\n",
    "                idx+=1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "traindata = MyDataset(dataset=dataset['train'], tokenizer=tokenizer)\n",
    "val_size = int(len(traindata) * 0.2)  \n",
    "train_size = len(traindata) - val_size  \n",
    "traindata, valdata = random_split(traindata, [train_size, val_size])\n",
    "testdata = MyDataset(dataset=dataset['test'], tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(traindata, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(valdata, batch_size=32)\n",
    "test_dataloader = DataLoader(testdata, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "n_heads = 8\n",
    "n_layers = 3\n",
    "vocab_size = 30522\n",
    "#vocab_size = 91015\n",
    "attn_drop_rate = 0.5\n",
    "layer_drop_rate = 0.2\n",
    "if df == \"asc\":\n",
    "    num_classes = 19\n",
    "elif df == \"dsc\":\n",
    "    num_classes = 10   \n",
    "elif df == \"news\":\n",
    "    num_classes = 20\n",
    "else:\n",
    "    raise Exception\n",
    "model = rememBERT(n_layers, vocab_size, embed_dim, n_heads, num_classes, attn_drop_rate, layer_drop_rate, seq_len)\n",
    "model = model.to(device)\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    correct = (predicted == labels).sum().item()  \n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    predicted = predicted.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    f1macro = f1_score(labels, predicted, average='macro')\n",
    "    return accuracy, f1macro, predicted, labels\n",
    "\n",
    "num_epochs = 40\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING VALIDATION LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    wandb.log({\"epoch\": epoch+1})\n",
    "    print(f\"######## Training Epoch {epoch + 1}/{num_epochs} #########\")\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    epoch_f1macro_pred = []\n",
    "    valid_epoch_f1macro_pred = []\n",
    "    all_labels = []\n",
    "    valid_labels = []\n",
    "    for batch_idx,batch in enumerate(train_dataloader,1):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        labels = batch['label']\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output, labels)\n",
    "        wandb.log({\"batch_loss\": loss})\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        accuracy, f1macro, predicted, macro_labels = calculate_accuracy(output, labels)\n",
    "        epoch_f1macro_pred.extend(predicted)\n",
    "        all_labels.extend(macro_labels)\n",
    "        total_correct += (accuracy * labels.size(0))\n",
    "        total_samples += labels.size(0)\n",
    "        wandb.log({\"batch_accuracy\": accuracy})\n",
    "        wandb.log({\"batch_f1macro\": f1macro})\n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = total_loss / batch_idx\n",
    "            print(f\"Batch {batch_idx}/{len(train_dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
    "            wandb.log({\"avg_batch_loss\": avg_loss})\n",
    "    \n",
    "    epoch_f1macro = f1_score(all_labels, epoch_f1macro_pred, average='macro')\n",
    "    print(f\"epoch_f1macro = {epoch_f1macro}\")\n",
    "    avg_epoch_loss = total_loss / len(train_dataloader)\n",
    "    training_accuracy = total_correct / total_samples\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} accuracy = {training_accuracy}\")\n",
    "    print(f\"Epoch {epoch + 1} - Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "    wandb.log({\"avg_epoch_loss\": avg_epoch_loss})\n",
    "    wandb.log({\"epoch_training_accuracy\": training_accuracy})\n",
    "    wandb.log({\"epoch_training_f1macro\": epoch_f1macro})\n",
    "    print(\"###### Validating ######\")\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "      for batch in val_dataloader:\n",
    "          input_ids = batch['input_ids'].squeeze(1)\n",
    "          labels = batch['label']\n",
    "          input_ids = input_ids.to(device)\n",
    "          labels = labels.to(device)\n",
    "          outputs = model(input_ids)\n",
    "          accuracy, f1macro, predicted, macro_labels = calculate_accuracy(outputs, labels)\n",
    "          valid_epoch_f1macro_pred.extend(predicted)\n",
    "          valid_labels.extend(macro_labels)\n",
    "          total_correct += (accuracy * labels.size(0))\n",
    "          total_samples += labels.size(0)\n",
    "    valid_f1macro = f1_score(valid_labels, valid_epoch_f1macro_pred, average='macro')\n",
    "    testing_accuracy = total_correct / total_samples\n",
    "    print(f\"Epoch {epoch + 1} - Validation Accuracy: {testing_accuracy:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} - Validation F1 Macro: {valid_f1macro}\")\n",
    "    wandb.log({\"validation_accuracy\": testing_accuracy})\n",
    "    wandb.log({\"validation_f1macro\": valid_f1macro})\n",
    "    # SAVE MODEL FOR EVERY EPOCH\n",
    "    # curent_state = {\n",
    "    #         'epoch': epoch + 1,\n",
    "    #         'model_state': model.state_dict(),\n",
    "    #         'optimizer_state': optimizer.state_dict(),\n",
    "    #     }\n",
    "    # save_path = f\"models/cross_attention_endoffset_best_model_fixed_v6.pth\"\n",
    "    # torch.save(curent_state, save_path)\n",
    "    # print(f\"Saved model state to'{save_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"###### Final Testing on Test data ######\")\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "test_epoch_f1macro_pred = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].squeeze(1)\n",
    "        labels = batch['label']\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        accuracy, f1macro, predicted, macro_labels = calculate_accuracy(outputs, labels)\n",
    "        test_epoch_f1macro_pred.extend(predicted)\n",
    "        test_labels.extend(macro_labels)\n",
    "        total_correct += (accuracy * labels.size(0))\n",
    "        total_samples += labels.size(0)\n",
    "test_f1macro = f1_score(test_labels, test_epoch_f1macro_pred, average='macro')\n",
    "testing_accuracy = total_correct / total_samples\n",
    "print(f\"Final Testing Accuracy: {testing_accuracy:.4f}\")\n",
    "print(f\"Epoch {epoch + 1} - Validation F1 Macro: {test_f1macro}\")\n",
    "wandb.log({\"avg_epoch_loss\": avg_epoch_loss})\n",
    "wandb.log({\"test_f1macro\": test_f1macro})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
